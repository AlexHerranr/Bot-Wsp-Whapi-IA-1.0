# üìã Funcionalidades Media para Bot WhatsApp

## üéØ Etapa 1: Detecci√≥n de Respuestas Citadas ‚úÖ

### Descripci√≥n
Esta funcionalidad detecta cuando un usuario responde a un mensaje espec√≠fico (quoted/reply) y enriquece el contexto agregando informaci√≥n del mensaje citado.

### Implementaci√≥n Completada

#### 1. Cambios en el C√≥digo

**Archivo: `src/utils/userStateManager.ts`**
- Agregados campos `lastInputVoice` y `quotedMessagesCount` a la interface `UserState`

**Archivo: `src/app-unified.ts`**
- Importado tipo `UserState` desde `userStateManager.js`
- Creado Map global `globalUserStates` para mantener estados de usuario
- Agregada funci√≥n helper `getTimestamp()`
- Implementada l√≥gica de detecci√≥n en el webhook handler

#### 2. Variables de Entorno Agregadas

En `.env` y `.env.example`:
```env
# === NUEVAS FUNCIONALIDADES MEDIA ===
ENABLE_REPLY_DETECTION=false
ENABLE_IMAGE_PROCESSING=false
ENABLE_VOICE_TRANSCRIPTION=false
ENABLE_VOICE_RESPONSES=false

# Configuraci√≥n adicional...
```

#### 3. C√≥mo Funciona

Cuando un usuario responde a un mensaje:
1. El webhook detecta el campo `context.quoted_content`
2. Extrae el texto del mensaje citado
3. Enriquece el mensaje actual con el contexto
4. Formato: `[Usuario responde a: "texto citado..."] mensaje actual`
5. El mensaje enriquecido se env√≠a a OpenAI con contexto completo

### Activaci√≥n

1. En tu archivo `.env`, cambia:
   ```env
   ENABLE_REPLY_DETECTION=true
   ```

2. Reinicia el bot:
   ```bash
   npm run dev
   ```

### Pruebas

#### Prueba Manual
1. En WhatsApp, env√≠a un mensaje al bot
2. Responde a ese mensaje usando la funci√≥n de responder/citar
3. Verifica en los logs que aparezca: `üì± Respuesta detectada a:`

#### Script de Prueba
```bash
node scripts/test-reply-detection.js
```

### Logs y Monitoreo

Los siguientes logs se generan:
- `QUOTED_MESSAGE_DETECTED` - Cuando se detecta una respuesta citada
- Console log con emoji üì± para identificaci√≥n visual r√°pida

### M√©tricas

El sistema rastrea:
- `quotedMessagesCount` - N√∫mero de respuestas citadas por usuario
- `lastMessageTime` - √öltima actividad del usuario

### Pr√≥ximas Etapas

- [ ] Etapa 2: Procesamiento de Im√°genes
- [ ] Etapa 3: Transcripci√≥n de Voz
- [ ] Etapa 4: Respuestas de Voz

### Troubleshooting

**Problema**: No se detectan las respuestas citadas
- Verifica que `ENABLE_REPLY_DETECTION=true` en `.env`
- Confirma que el webhook est√° recibiendo el campo `context`
- Revisa los logs para mensajes de error

**Problema**: El contexto no se agrega correctamente
- Verifica la estructura del mensaje en los logs
- Aseg√∫rate de que el mensaje citado tenga contenido de texto

---

## üéØ Etapa 3: Transcripci√≥n de Voz ‚úÖ

### Descripci√≥n
Esta funcionalidad transcribe autom√°ticamente las notas de voz enviadas por los usuarios usando OpenAI Whisper API, convirtiendo el audio en texto para que el bot pueda procesarlo y responder adecuadamente.

### Implementaci√≥n Completada

#### 1. Cambios en el C√≥digo

**Archivo: `src/app-unified.ts`**
- Agregada funci√≥n `transcribeAudio()` para procesar audio con Whisper API
- Implementado manejo de mensajes tipo 'voice', 'audio' y 'ptt' (Push to Talk)
- Transcripci√≥n as√≠ncrona con fallbacks para errores
- Marcado de estado `lastInputVoice` para respuestas contextuales

#### 2. C√≥mo Funciona

Cuando un usuario env√≠a una nota de voz:
1. El webhook detecta mensaje tipo 'voice', 'audio' o 'ptt'
2. Extrae la URL del audio
3. Descarga el audio y valida el tama√±o (m√°x. 25MB)
4. Env√≠a a OpenAI Whisper API (modelo whisper-1)
5. Recibe transcripci√≥n en espa√±ol
6. Agrega al buffer: `üé§ [transcripci√≥n del audio]`
7. Marca `lastInputVoice = true` para posibles respuestas de voz

### Configuraci√≥n

Variables de entorno relevantes:
```env
ENABLE_VOICE_TRANSCRIPTION=false    # Toggle principal
WHISPER_LANGUAGE=es                 # Idioma de transcripci√≥n
MAX_AUDIO_SIZE=26214400            # 25MB m√°ximo
MAX_AUDIO_DURATION=300             # 5 minutos m√°ximo
```

### Activaci√≥n

1. En tu archivo `.env`, cambia:
   ```env
   ENABLE_VOICE_TRANSCRIPTION=true
   ```

2. Reinicia el bot

### Pruebas

#### Prueba Manual
1. Env√≠a una nota de voz desde WhatsApp
2. Verifica en los logs: `üé§ Procesando nota de voz...`
3. Confirma que aparezca: `üé§ Transcripci√≥n: "[texto transcrito]"`
4. El bot debe responder considerando el contenido transcrito

#### Script de Prueba
```bash
node scripts/test-voice-transcription.js
```

Este script prueba:
- Nota de voz normal (type: voice)
- Audio PTT (Push to Talk)
- Audio sin URL (fallback)

### Logs y Monitoreo

Logs generados:
- `AUDIO_TRANSCRIPTION_ERROR` - Error transcribiendo audio
- `AUDIO_NO_URL` - Audio recibido sin URL
- `VOICE_TRANSCRIBED` - Voz transcrita exitosamente
- `VOICE_PROCESSING_ERROR` - Error general en procesamiento
- Console logs con emoji üé§ para identificaci√≥n visual

### Optimizaciones

- Temperatura 0.2 para mayor precisi√≥n
- Prompt contextualizado para hoteles
- Procesamiento as√≠ncrono para no bloquear
- Validaci√≥n de tama√±o antes de descargar

### Tipos de Audio Soportados

1. **voice** - Notas de voz normales
2. **audio** - Archivos de audio
3. **ptt** - Push to Talk (grabaci√≥n r√°pida)

### Casos de Uso

Ideal para:
- Consultas r√°pidas por voz mientras conducen
- Usuarios que prefieren hablar en lugar de escribir
- Mensajes largos o detallados
- Accesibilidad para usuarios con dificultades para escribir

### M√©tricas Rastreadas

- `transcriptionLength` - Longitud del texto transcrito
- `words` - N√∫mero de palabras transcritas
- `lastInputVoice` - Marca para respuestas contextuales

### Troubleshooting

**Problema**: Las notas de voz no se transcriben
- Verifica `ENABLE_VOICE_TRANSCRIPTION=true`
- Confirma que la API key tenga acceso a Whisper
- Revisa el tama√±o del audio (< 25MB)

**Problema**: Transcripciones en ingl√©s
- Verifica `WHISPER_LANGUAGE=es` en .env
- El prompt est√° en espa√±ol para contexto

**Problema**: Errores de timeout
- Audios muy largos pueden causar timeout
- Considera reducir `MAX_AUDIO_DURATION`

**Problema**: Transcripciones imprecisas
- Whisper funciona mejor con audio claro
- Ruido de fondo puede afectar la precisi√≥n

### Pr√≥ximas Etapas

- [x] Etapa 1: Detecci√≥n de Respuestas Citadas
- [x] Etapa 2: Procesamiento de Im√°genes
- [x] Etapa 3: Transcripci√≥n de Voz
- [x] Etapa 4: Respuestas de Voz

---

## üéØ Etapa 4: Respuestas de Voz Autom√°ticas ‚úÖ

### Descripci√≥n
Esta funcionalidad genera autom√°ticamente respuestas en voz usando OpenAI TTS (Text-to-Speech), decidiendo inteligentemente cu√°ndo usar voz bas√°ndose en el contexto de la conversaci√≥n.

### Implementaci√≥n Completada

#### 1. Cambios en el C√≥digo

**Archivo: `src/app-unified.ts`**
- Modificada funci√≥n `sendWhatsAppMessage()` para incluir l√≥gica de decisi√≥n de voz
- Implementada generaci√≥n de audio con OpenAI TTS
- Conversi√≥n y env√≠o de audio como nota de voz via WHAPI
- Fallback autom√°tico a texto si falla el env√≠o de voz

#### 2. C√≥mo Funciona

El bot decide usar voz cuando:
1. **Usuario envi√≥ voz** - Si `lastInputVoice = true`
2. **Mensaje largo** - Si supera `VOICE_THRESHOLD` caracteres
3. **Respuesta a transcripci√≥n** - Si el mensaje contiene emoji üé§
4. **Factor aleatorio** - Probabilidad configurable para variedad

Proceso:
1. Eval√∫a criterios de decisi√≥n
2. Limpia el texto (emojis, caracteres especiales)
3. Genera audio con OpenAI TTS
4. Convierte a base64
5. Env√≠a como nota de voz via WHAPI
6. Fallback a texto si hay error

### Configuraci√≥n

Variables de entorno relevantes:
```env
ENABLE_VOICE_RESPONSES=false       # Toggle principal
TTS_VOICE=alloy                   # Voz a usar
VOICE_THRESHOLD=150               # Caracteres m√≠nimos
VOICE_RANDOM_PROBABILITY=0.1      # 10% probabilidad aleatoria
```

Voces disponibles:
- **alloy** - Neutral, balanceada
- **echo** - Masculina, profunda
- **fable** - Brit√°nica, expresiva
- **onyx** - Masculina, grave
- **nova** - Femenina, c√°lida
- **shimmer** - Femenina, suave

### Activaci√≥n

1. En tu archivo `.env`:
   ```env
   ENABLE_VOICE_RESPONSES=true
   TTS_VOICE=nova  # O tu preferida
   ```

2. Reinicia el bot

### Pruebas

#### Prueba Manual
1. **Test voz‚Üívoz**: Env√≠a una nota de voz, deber√≠as recibir respuesta en voz
2. **Test mensaje largo**: Env√≠a mensaje > 150 caracteres
3. **Test mensaje corto**: Env√≠a mensaje < 150 caracteres (respuesta en texto)

#### Script de Prueba
```bash
node scripts/test-voice-responses.js
```

Este script prueba:
- Entrada de voz ‚Üí Respuesta de voz
- Mensaje largo ‚Üí Respuesta de voz
- Mensaje corto ‚Üí Respuesta de texto

### Logs y Monitoreo

Logs generados:
- `VOICE_RESPONSE_SENT` - Respuesta de voz enviada exitosamente
- `VOICE_SEND_ERROR` - Error enviando voz (fallback a texto)
- Console logs con emoji üîä para identificaci√≥n visual

### Optimizaciones

- Limpieza autom√°tica de emojis y caracteres especiales
- L√≠mite de 4096 caracteres para TTS
- Fallback robusto a texto si falla
- Flag `lastInputVoice` se limpia despu√©s de responder

### Criterios de Decisi√≥n

1. **Prioridad alta**: Usuario envi√≥ voz
2. **Prioridad media**: Mensaje largo o contiene üé§
3. **Prioridad baja**: Factor aleatorio

### Casos de Uso

Ideal para:
- Respuestas a consultas por voz
- Mensajes largos o explicaciones detalladas
- Crear experiencia m√°s conversacional
- Accesibilidad para usuarios con dificultades visuales

### Costos

- TTS-1: $0.015 por 1K caracteres
- Aproximadamente $0.05-0.15 por respuesta de voz t√≠pica
- Controlable via thresholds y probabilidad

### Troubleshooting

**Problema**: No se generan respuestas de voz
- Verifica `ENABLE_VOICE_RESPONSES=true`
- Confirma que OpenAI API key tenga acceso a TTS
- Revisa el threshold configurado

**Problema**: Voz suena rob√≥tica o poco natural
- Prueba diferentes voces (nova, alloy, echo)
- Ajusta la velocidad si es necesario

**Problema**: Demasiadas respuestas de voz
- Aumenta `VOICE_THRESHOLD` (ej: 200-300)
- Reduce `VOICE_RANDOM_PROBABILITY` (ej: 0.05)

**Problema**: Error al enviar audio
- Verifica l√≠mites de WHAPI para audio
- Revisa que el formato base64 sea correcto